{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Get the required packages and libraries\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from keras.preprocessing import image\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.layers.core import Activation, Flatten, Dropout, Dense\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "import matplotlib.image as mpimg\n",
    "from sklearn.preprocessing import LabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declaration of variables\n",
    "\n",
    "proportion = 0.1\n",
    "BATCH_SIZE = 20\n",
    "SIZE = 150\n",
    "DEFAULT_IMAGE_SIZE = tuple((150,150))\n",
    "EPOCH = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # To take a proportion of whole data\n",
    "# # Get train and val data\n",
    "# data_train = os.listdir('D:/POST_S1/BANGKIT/TomatoLeaf/tomato/train_lite/')\n",
    "# data_val = os.listdir('D:/POST_S1/BANGKIT/TomatoLeaf/tomato/test_lite/')\n",
    "\n",
    "# # Get a proportion of whole data\n",
    "# for folder in data_train:\n",
    "#     i = 0\n",
    "#     for data in os.listdir('tomato/val/'+folder):\n",
    "#         if(i < proportion * len(os.listdir('tomato/val/'+folder))):\n",
    "#             shutil.copyfile('tomato/val/'+folder+'/'+data, 'tomato/test_lite/'+folder+'/'+data)\n",
    "#             i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the base directory (Change according to your data)\n",
    "base_dir = 'D:/POST_S1/BANGKIT/TomatoLeaf/tomato/'\n",
    "\n",
    "train_dir = base_dir + 'train_lite/'\n",
    "test_dir = base_dir + 'test_lite/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1000 images belonging to 10 classes.\n",
      "Found 100 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "# Make ImageGenerator object as argument of fit function\n",
    "\n",
    "# All images will be rescaled by 1./255.\n",
    "train_datagen = ImageDataGenerator( \n",
    "                    rescale = 1.0/255. ,\n",
    "                    rotation_range=40,\n",
    "                    width_shift_range=0.4,\n",
    "                    height_shift_range=0.4,\n",
    "                    shear_range=0.2,\n",
    "                    zoom_range=0.2,\n",
    "                    horizontal_flip=True,\n",
    "                    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "test_datagen  = ImageDataGenerator( rescale = 1.0/255. )\n",
    "\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(train_dir,\n",
    "                                                    batch_size=BATCH_SIZE,\n",
    "                                                    class_mode='categorical',\n",
    "                                                    target_size=DEFAULT_IMAGE_SIZE)\n",
    "\n",
    "test_generator =  test_datagen.flow_from_directory(test_dir,\n",
    "                                                         batch_size=BATCH_SIZE,\n",
    "                                                         class_mode  = 'categorical',\n",
    "                                                         target_size = DEFAULT_IMAGE_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the layers of model\n",
    "\n",
    "#chanDim = -1\n",
    "model = tf.keras.models.Sequential([\n",
    "    # Note the input shape is the desired size of the image 150x150 with 3 bytes color\n",
    "    tf.keras.layers.Conv2D(128, (3,3), activation='relu', input_shape=(SIZE, SIZE, 3)),\n",
    "    tf.keras.layers.BatchNormalization(axis=-1),\n",
    "    tf.keras.layers.MaxPooling2D(3,3),\n",
    "    tf.keras.layers.Dropout(0.25),\n",
    "    tf.keras.layers.Conv2D(128, (3,3), activation='relu'),\n",
    "    tf.keras.layers.BatchNormalization(axis=-1),\n",
    "    tf.keras.layers.Conv2D(128, (3,3), activation='relu'),\n",
    "    tf.keras.layers.BatchNormalization(axis=-1),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    tf.keras.layers.Dropout(0.25),\n",
    "    tf.keras.layers.Conv2D(128, (3,3), activation='relu'), \n",
    "    tf.keras.layers.BatchNormalization(axis=-1),\n",
    "    tf.keras.layers.Conv2D(128, (3,3), activation='relu'), \n",
    "    tf.keras.layers.BatchNormalization(axis=-1),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    tf.keras.layers.Dropout(0.25),\n",
    "    # Flatten the results to feed into a DNN\n",
    "    tf.keras.layers.Flatten(), \n",
    "    # 512 neuron hidden layer\n",
    "    tf.keras.layers.Dense(1024, activation='relu'), \n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    # Only 1 output neuron. It will contain a value from 0-1 where 0 for 1 class ('cats') and 1 for the other ('dogs')\n",
    "    tf.keras.layers.Dense(10, activation='softmax')  \n",
    "\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VGG Architecture\n",
    "model = Sequential()\n",
    "model.add(Conv2D(input_shape=(SIZE,SIZE,3),filters=64,kernel_size=(3,3),padding=\"same\", activation=\"relu\"))\n",
    "model.add(Conv2D(filters=64,kernel_size=(3,3),padding=\"same\", activation=\"relu\"))\n",
    "model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n",
    "model.add(Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "model.add(Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n",
    "model.add(Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "model.add(Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "model.add(Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n",
    "model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n",
    "model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(units=4096,activation=\"relu\"))\n",
    "model.add(Dense(units=4096,activation=\"relu\"))\n",
    "model.add(Dense(units=2, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_5 (Conv2D)            (None, 148, 148, 128)     3584      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 49, 49, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 47, 47, 128)       147584    \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 45, 45, 128)       147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 22, 22, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 20, 20, 128)       147584    \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 18, 18, 128)       147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 9, 9, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 10368)             0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1024)              10617856  \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                10250     \n",
      "=================================================================\n",
      "Total params: 11,222,026\n",
      "Trainable params: 11,222,026\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Show the architecture of layers\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model with optimizer\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "50/50 [==============================] - 126s 3s/step - loss: 2.3198 - accuracy: 0.0930 - val_loss: 2.2995 - val_accuracy: 0.1400\n",
      "Epoch 2/5\n",
      "50/50 [==============================] - 101s 2s/step - loss: 2.2442 - accuracy: 0.1540 - val_loss: 2.1846 - val_accuracy: 0.1700\n",
      "Epoch 3/5\n",
      "50/50 [==============================] - 81s 2s/step - loss: 2.1692 - accuracy: 0.1830 - val_loss: 1.9216 - val_accuracy: 0.3300\n",
      "Epoch 4/5\n",
      "50/50 [==============================] - 82s 2s/step - loss: 2.0884 - accuracy: 0.2090 - val_loss: 2.1170 - val_accuracy: 0.2100\n",
      "Epoch 5/5\n",
      "50/50 [==============================] - 82s 2s/step - loss: 1.9501 - accuracy: 0.2500 - val_loss: 1.9652 - val_accuracy: 0.2900\n"
     ]
    }
   ],
   "source": [
    "# Train data with defined model\n",
    "\n",
    "history = model.fit(train_generator,\n",
    "                    validation_data=test_generator,\n",
    "                    steps_per_epoch=len(train_generator),\n",
    "                    validation_steps =len(test_generator),\n",
    "                    epochs=EPOCH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training result\n",
    "\n",
    "fig, ax = plt.subplots(1,2,figsize=(18,3))\n",
    "ax[0].plot(history.history[\"accuracy\"])\n",
    "ax[0].plot(history.history[\"val_accuracy\"])\n",
    "ax[0].set_title(\"Train and Validation Accuracy\")\n",
    "ax[0].set_xlabel(\"Epochs\")\n",
    "ax[0].set_ylabel(\"Accuracy\")\n",
    "ax[0].legend(['Train', 'Validation'], loc='upper right')\n",
    "\n",
    "ax[1].plot(history.history[\"loss\"])\n",
    "ax[1].plot(history.history[\"val_loss\"])\n",
    "ax[1].set_title(\"Train and Validation Loss\")\n",
    "ax[1].set_xlabel(\"Epochs\")\n",
    "ax[1].set_ylabel(\"Loss\")\n",
    "ax[1].legend([\"Train\", \"Validation\"], loc=\"upper right\")\n",
    "plt.savefig(datetime.now().strftime(\"%m%d%Y_%H%M%S\") + '.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save trained model into .h5 format\n",
    "\n",
    "model.save('model1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00,\n",
       "        0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00,\n",
       "        1.000000e+00, 4.679783e-21]], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predict the image using defined model\n",
    "\n",
    "img=image.load_img('sehat.jpg', target_size=(150, 150))\n",
    "x=image.img_to_array(img)\n",
    "x=np.expand_dims(x, axis=0)\n",
    "images = np.vstack([x])\n",
    "\n",
    "model.predict(images, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Tensorflowjs\n",
    "#!pip install tensorflowjs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert .h5 model into model.json\n",
    "!tensorflowjs_converter --input_format keras model1.h5 model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
